# -*- coding: utf-8 -*-
"""Fish_diseases_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h_f6GAr-AxhCqj0DTV5O_GJd3ZAfY28R
"""

# install missing packages
!pip -q install torchsummary

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib
import os
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset, random_split
from torchvision.datasets import ImageFolder
from torchvision.utils import make_grid
from torchsummary import summary
from tqdm import tqdm
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from pathlib import Path
import re


# set background color to white
matplotlib.rcParams['figure.facecolor'] = '#ffffff'

# set default figure size
matplotlib.rcParams['figure.figsize'] = (15, 7)

from google.colab import drive

drive.mount('/content/drive') # Mount your Google Drive first

# Update this path to where your dataset is in Google Drive
image_dir_train = Path('/content/drive/MyDrive/fish_Classification/train_split')



# Create a list of all filepaths
filepaths_train = []
expected_extensions_train = ['jpg', 'jpeg', 'png', 'bmp', 'tiff'] # List of all expected extensions
for ext_train in expected_extensions_train:
    filepaths_train.extend(list(image_dir_train.glob(f'**/*.{ext_train}'))) # Search for each extension

#filepaths = list(image_dir.glob(r'**/*.jpg'))
labels_train = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths_train))
filepaths_train = pd.Series(filepaths_train, name='Filepath_train').astype(str)
labels_train = pd.Series(labels_train, name='Label_train')
image_df_train = pd.concat([filepaths_train, labels_train], axis=1)

print(image_df_train.head())
print(f"Total images found for train: {len(image_df_train)}")

import pandas as pd # Make sure pandas is imported
from pathlib import Path # <--- THIS IS THE MISSING IMPORT HERE
from google.colab import drive

drive.mount('/content/drive') # Mount your Google Drive first

# Update this path to where your dataset is in Google Drive
image_dir_test = Path('/content/drive/MyDrive/fish_Classification/test_split ')


# Create a list of all filepaths
filepaths_test = []
expected_extensions = ['jpg', 'jpeg', 'png', 'bmp', 'tiff'] # Use the same list name for consistency
for ext in expected_extensions: # Use 'ext' here, consistent with the list name
    filepaths_test.extend(list(image_dir_test.glob(f'**/*.{ext}')))

labels_test = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths_test))
image_df_test = pd.DataFrame({'Filepath': filepaths_test, 'Label_test': labels_test}) # Use direct DataFrame creation for conciseness

print(image_df_test.head())
print(f"Total images found for test: {len(image_df_test)}")

image_df_train

image_df_test

# count plot for each class
plt.figure(figsize=(12, 6)) # Make the plot wider for better spacing
sns.countplot(x='Label_train', data=image_df_train, color='steelblue').set(title='Count of different image classes for train data')
plt.xticks(rotation=45, ha='right') # Rotate labels by 45 degrees, align them to the right
plt.tight_layout() # Adjust layout to prevent labels from being cut off
plt.show()

# count plot for each class
plt.figure(figsize=(12, 6)) # Make the plot wider for better spacing
sns.countplot(x='Label_test', data=image_df_test, color='steelblue').set(title='Count of different image classes for test data')
plt.xticks(rotation=45, ha='right') # Rotate labels by 45 degrees, align them to the right
plt.tight_layout() # Adjust layout to prevent labels from being cut off
plt.show()

#augumentation
train_transforms = transforms.Compose([
    transforms.Resize([128, 128]),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomVerticalFlip(p=0.5),
    transforms.RandomRotation(degrees=30),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.RandomGrayscale(p=0.1),
    transforms.RandomPerspective(distortion_scale=0.2, p=0.3),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

val_test_transforms = transforms.Compose([
    transforms.Resize([128, 128]),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

import torch
from torch.utils.data import Dataset, random_split
from torchvision import transforms
from torchvision.datasets import ImageFolder

# Custom Wrapper Dataset
class TransformedSubset(Dataset):
    def __init__(self, subset, transform=None):
        self.subset = subset
        self.transform = transform

    def __getitem__(self, index):
        x, y = self.subset[index]
        if self.transform:
            x = self.transform(x)
        return x, y

    def __len__(self):
        return len(self.subset)

image_dir_train = Path('/content/drive/MyDrive/fish_Classification/train_split')
initial_dataset = ImageFolder(root=image_dir_train)

initial_dataset.classes

#Perform the random split on the untransformed data
size = len(initial_dataset)
val_size = int(0.2 * size)
train_size = int(size - val_size)
print(f"number of classes: {len(initial_dataset.classes)}")
print(f"total number of images: {size}")
print(f"total number of train images: {train_size}")
print(f"total number of validation images: {val_size}")
generator = torch.Generator().manual_seed(42)
train_subset, val_subset = random_split(initial_dataset, [train_size, val_size], generator=generator)

train_set = TransformedSubset(train_subset, transform=train_transforms)
val_set = TransformedSubset(val_subset, transform=val_test_transforms)

image_dir_test = Path('/content/drive/MyDrive/fish_Classification/test_split ')
test_set = ImageFolder(root=image_dir_test, transform=val_test_transforms)

# show a single image
def show_image(img, label, dataset):
    plt.imshow(img.permute(1, 2, 0))
    plt.axis('off')
    plt.title(dataset.classes[label])

show_image(*train_set[0], initial_dataset)

# --- 4. Create DataLoaders ---
batch_size = 16
train_dl = DataLoader(train_set, batch_size=batch_size, shuffle=True)
val_dl = DataLoader(val_set, batch_size=batch_size)
test_dl = DataLoader(test_set, batch_size=batch_size) # Renamed for clarity

# visualize a batch of images
def show_batch(dl):
    for images, labels in dl:
        fig, ax = plt.subplots(figsize=(20, 8))
        ax.set_xticks([]); ax.set_yticks([])
        ax.imshow(make_grid(images, nrow=16).permute(1, 2, 0))
        break

show_batch(train_dl)

show_batch(val_dl)

show_batch(test_dl)

import torch.nn as nn

# The conv_block is correct.
def conv_block(in_channels, out_channels, pool=False):
    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
              nn.BatchNorm2d(out_channels),
              nn.ReLU(inplace=True)]
    if pool: layers.append(nn.MaxPool2d(2))
    return nn.Sequential(*layers)


# --- CORRECTED Custom CNN ---
class FishCustomNet(nn.Module):
    def __init__(self, in_channels, num_classes):
        super().__init__()

        # --- Convolutional Base (Unchanged) ---
        self.conv1 = conv_block(in_channels, 64)     # Output: 64 x 128 x 128 because pool = false by default
        self.conv2 = conv_block(64, 128, pool=True)  # Output: 128 x 64 x 64
        self.conv3 = conv_block(128, 256, pool=True) # Output: 256 x 32 x 32
        self.conv4 = conv_block(256, 512, pool=True) # Output: 512 x 16 x 16

        # --- Classifier (Fully Connected Layers with BatchNorm1d) ---
        self.classifier = nn.Sequential(
            nn.MaxPool2d(2),
            nn.Flatten(),
            nn.Dropout(0.2),

            # --- Block 1 of the Classifier ---
            nn.Linear(512 * 8 * 8, 1024),
            nn.BatchNorm1d(1024),  # Add BatchNorm1d for the 1024 features
            nn.ReLU(inplace=True),

            # --- Block 2 of the Classifier ---
            nn.Linear(1024, 512),
            nn.BatchNorm1d(512),   # Add BatchNorm1d for the 512 features
            nn.ReLU(inplace=True),

            # --- Final Output Layer ---
            # Typically, we do NOT add BatchNorm or ReLU to the final output layer.
            # The raw output (logits) is needed for the cross-entropy loss function.
            nn.Linear(512, num_classes)
        )

    def forward(self, xb):
        out = self.conv1(xb)
        out = self.conv2(out)
        out = self.conv3(out)
        out = self.conv4(out)
        out = self.classifier(out)
        return out

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # choose device accordingly
model = FishCustomNet(3, 8).to(device) # 3 color channels and 8 output classes
criterion = nn.CrossEntropyLoss()
optim = torch.optim.Adam(model.parameters(), lr=1e-3)

# model summary (helps in understanding the output shapes)
summary(model, (3, 128, 128))

# multiclass accuracy
def multi_acc(y_pred, y_test):
    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)
    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)
    correct_pred = (y_pred_tags == y_test).float()
    acc = correct_pred.sum() / len(correct_pred)
    #acc = torch.round(acc * 100)
    return acc

# training loop
epochs = 100
history = []

for epoch in range(epochs):
    # --- Training Phase ---
    model.train()
    running_train_loss = 0.0
    running_train_acc = 0.0

    with tqdm(train_dl, unit="batch") as tepoch:
        for data, target in tepoch:
            tepoch.set_description(f"Epoch {epoch + 1}")
            data, target = data.to(device), target.to(device)

            optim.zero_grad()
            out = model(data)
            loss = criterion(out, target)
            acc = multi_acc(out, target) # Assuming this now returns a fraction (e.g., 0.75)
            loss.backward()
            optim.step()

            running_train_loss += loss.item()
            running_train_acc += acc.item()

            # --- THE ONLY CHANGE IS HERE ---
            # Multiply by 100 to correctly format the percentage for display
            tepoch.set_postfix(loss=loss.item(), accuracy=f"{acc.item()*100:.2f}%")

    # --- Validation Phase ---
    model.eval()
    running_val_loss = 0.0
    running_val_acc = 0.0

    with torch.no_grad():
        for data, target in val_dl:
            data, target = data.to(device), target.to(device)
            out = model(data)
            loss = criterion(out, target)
            acc = multi_acc(out, target)
            running_val_loss += loss.item()
            running_val_acc += acc.item()

    # --- Calculate and print epoch averages ---
    avg_train_loss = running_train_loss / len(train_dl)
    avg_train_acc = running_train_acc / len(train_dl)
    avg_val_loss = running_val_loss / len(val_dl)
    avg_val_acc = running_val_acc / len(val_dl)

    # Store results for plotting
    history.append({
        'train_loss': avg_train_loss,
        'val_loss': avg_val_loss,
        'train_acc': avg_train_acc,
        'val_acc': avg_val_acc
    })

    # Print epoch summary
    print(f"Epoch {epoch+1}/{epochs} -> "
          f"Training Loss: {avg_train_loss:.4f}, "
          f"Validation Loss: {avg_val_loss:.4f}, "
          f"Training Accuracy: {avg_train_acc*100:.2f}%, "
          f"Validation Accuracy: {avg_val_acc*100:.2f}%")

history_df = pd.DataFrame(history)

# Set the style for the plots
sns.set_style("whitegrid")

# --- Plot 1: Training Loss vs. Validation Loss ---
plt.figure(figsize=(10, 5))
sns.lineplot(data=history_df[['train_loss', 'val_loss']])
plt.title("Loss vs. Epochs")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend(['Training Loss', 'Validation Loss'])
plt.show()


# --- Plot 2: Training Accuracy vs. Validation Accuracy ---
plt.figure(figsize=(10, 5))
# We multiply by 100 to show accuracy as a percentage
sns.lineplot(data=history_df[['train_acc', 'val_acc']] * 100)
plt.title("Accuracy vs. Epochs")
plt.xlabel("Epoch")
plt.ylabel("Accuracy (%)")
plt.legend(['Training Accuracy', 'Validation Accuracy'])
plt.show()

# predict on testing data samples (the accuracy here is batch accuracy)
y_pred_list = []
y_true_list = []
with torch.no_grad():
    with tqdm(test_dl, unit="batch") as tepoch:
        for inp, labels in tepoch:
            inp, labels = inp.to(device), labels.to(device)
            y_test_pred = model(inp)
            acc = multi_acc(y_test_pred, labels)
            _, y_pred_tag = torch.max(y_test_pred, dim = 1)
            tepoch.set_postfix(accuracy = acc.item())
            y_pred_list.append(y_pred_tag.cpu().numpy())
            y_true_list.append(labels.cpu().numpy())

# flatten prediction and true lists
flat_pred = []
flat_true = []
for i in range(len(y_pred_list)):
    for j in range(len(y_pred_list[i])):
        flat_pred.append(y_pred_list[i][j])
        flat_true.append(y_true_list[i][j])

print(f"number of testing samples results: {len(flat_pred)}")

# calculate total testing accuracy
print(f"Testing accuracy is: {accuracy_score(flat_true, flat_pred) * 100:.2f}%")

# You need to run this cell first to get all predictions
y_true = []
y_pred = []

model.eval()
with torch.no_grad():
    for data, target in test_dl: # Use the test DataLoader
        data, target = data.to(device), target.to(device)
        out = model(data)

        # Get predictions
        y_pred_softmax = torch.log_softmax(out, dim = 1)
        _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)

        y_pred.extend(y_pred_tags.cpu().numpy())
        y_true.extend(target.cpu().numpy())

# y_pred is now a list/array of all predicted labels for the test set
# Let's call it flat_pred as in your code
flat_pred = np.array(y_pred)

# --- ADD THIS HELPER FUNCTION TO DE-NORMALIZE ---
def denormalize(img_tensor):
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    # The permute is needed to align dimensions for broadcasting
    img_tensor = img_tensor.permute(1, 2, 0)
    img_tensor = img_tensor * std + mean
    # Clip values to be between 0 and 1
    img_tensor = np.clip(img_tensor, 0, 1)
    return img_tensor

# --- YOUR PLOTTING CODE (with the denormalize call added) ---
inds = np.random.randint(len(test_set), size=15)
fig, axes = plt.subplots(nrows=3, ncols=5, figsize=(15, 10), # Made figure taller
                        subplot_kw={'xticks': [], 'yticks': []})

for i, ax in zip(inds, axes.flat):
    img, label = test_set[i]

    # ADDED: De-normalize the image before plotting
    img_display = denormalize(img)

    ax.imshow(img_display)

    # This logic was correct, but I'll add a color code for clarity
    true_label = test_set.classes[label]
    pred_label = test_set.classes[flat_pred[i]]

    title_color = 'green' if true_label == pred_label else 'red'

    ax.set_title(f"True: {true_label}\nPredicted: {pred_label}", color=title_color)

plt.tight_layout()
plt.show()

print(classification_report(y_true, y_pred, target_names=initial_dataset.classes))

# plot confusion matrix
plt.figure(figsize=(7, 6))
idx2class = {v: k for k, v in initial_dataset.class_to_idx.items()}
confusion_matrix_df = pd.DataFrame(confusion_matrix(y_true, y_pred)).rename(columns=idx2class, index=idx2class)
sns.heatmap(confusion_matrix_df, annot=True, fmt='g').set(title="Confusion Matrix", xlabel="Predicted Label", ylabel="True Label")
plt.show()

torch.save(model.state_dict(), 'fish_classifier_model.pth')

torch.save(model.state_dict(), '/content/drive/MyDrive/fish_Classification/fish_classifier_model.pth')